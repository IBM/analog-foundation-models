adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1.0e-06
base_model: meta-llama/Llama-3.2-1B-Instruct
batch_size: 1
bf16: false
clip_sigma: 2.5
clip_type: gaussian_channel
config: ./data/training_files/meta-llama/Llama-3.2-1B-Instruct/config.yaml
distillation: true
distillation_beta: 1.0
distillation_temperature: 1.0
do_eval: true
do_train: true
ds_config_path: ./data/training_files/meta-llama/Llama-3.2-1B-Instruct/ds_config.json
eval_batch_size: 1
eval_medqa: true
eval_on_start: true
eval_only: false
eval_steps: 500
eval_strategy: 'no'
exclude_modules:
- base_layer
- lora_A
- lora_B
forward_inp_res: 254
forward_out_bound: 14
forward_out_noise: 0.0
forward_out_noise_per_channel: false
forward_out_res: 254
fp: false
gradient_accumulation_steps: 1
gradient_checkpointing: true
greater_is_better: false
init_lora_weights: true
input_range_decay: 0.01
input_range_enable: true
input_range_fast_mode: false
input_range_init_from_data: 500
input_range_init_std_alpha: 18.0
input_range_init_value: 3.0
input_range_init_with_max: false
input_range_input_min_percentage: 0.95
input_range_learn_input_range: true
load_best_model_at_end: false
logging_steps: 31
logging_strategy: steps
lora_alpha: 32
lora_bias: none
lora_dropout: 0.0
lora_enable: false
lora_r: 0
lora_target_modules:
- qkv_proj
- o_proj
- gate_up_proj
- down_proj
lr: 5.0e-07
lr_end: null
lr_scheduler_type: polynomial
mapping_max_input_size: -1
max_grad_norm: 1.0
max_steps: -1
modifier_enable_during_test: false
modifier_offset: 0.0
modifier_res: -1
modifier_std_dev: 0.03
modifier_type: add_gauss_channel
modules_to_save:
- lm_head
- norm
- post_attention_layernorm
- input_layernorm
- embed_tokens
num_gpus: 8
num_train_epochs: 2
output_dir: ./data/analog_llama
overwrite_output_dir: false
path_to_pretrained: ./data/meta-llama/Llama-3.2-1B-Instruct
pre_train_dataset: ./data/processed-data/meta-llama/Llama-3.2-1B-Instruct
report_to: wandb
save_steps: 300
save_strategy: steps
save_total_limit: 1
seed: 0
task_name: pretrain
use_dora: false
use_rslora: false
warmup_ratio: 0.016
weight_decay: 0.01
